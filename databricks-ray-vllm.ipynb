{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ray vLLM Multi-Node Inference on Databricks (AWS)\n",
    "# MAGIC\n",
    "This notebook shows how to run large language model (LLM) inference at scale using Ray and vLLM on Databricks with AWS A10 GPUs. It uses Databricks serverless GPU infrastructure to automatically provision and manage resources for distributed inference.\n",
    "# MAGIC\n",
    "Original Author of the notebook: Puneet Jain https://www.linkedin.com/in/puneetjain159/\n",
    "# MAGIC\n",
    "Key steps:\n",
    "- Install all required packages for Ray and vLLM distributed inference.\n",
    "- Authenticate securely with Hugging Face for model access.\n",
    "- Use Ray to launch and manage distributed workers across multiple GPUs.\n",
    "- Run text inference pipelines with efficient batching and parallelism.\n",
    "- Monitor Ray cluster resources to ensure optimal usage.\n",
    "- Follow workspace policies for resource management, security, and cleanup.\n",
    "# MAGIC\n",
    "All compute is provisioned on-demand and cleaned up automatically, making it easy to scale up or down without manual cluster management.\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## Package Installation\n",
    "# MAGIC\n",
    "Installs all required packages for distributed Ray and vLLM inference on Databricks (AWS):\n",
    "- Flash Attention (CUDA 12, PyTorch 2.6, Python 3.12, A10 GPU compatible)\n",
    "- Databricks Connect for Spark integration\n",
    "- Transformers <4.54.0, vLLM 0.8.5.post1\n",
    "- OpenTelemetry Prometheus exporter, optree, hf_transfer, numpy\n",
    "- Restarts Python for a clean environment\n",
    "# MAGIC\n",
    "All versions are pinned for compatibility and reproducibility.\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "%pip install --force-reinstall --no-cache-dir --no-deps \"https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl\"\n",
    "%pip install databricks-connect<16\n",
    "%pip install \"transformers<4.54.0\"\n",
    "%pip install \"vllm==0.8.5.post1\"\n",
    "%pip install  \"opentelemetry.exporter.prometheus\" \n",
    "%pip install  'optree>=0.13.0'\n",
    "%pip install  hf_transfer\n",
    "%pip install  \"numpy==1.26.4\"\n",
    "%restart_python\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# MAGIC\n",
    "## Version Verification\n",
    "**Purpose**: Verify package versions and enforce compatibility requirements\n",
    "# MAGIC\n",
    "**Key Actions**:\n",
    "* Prints versions of torch, flash_attn, vllm, ray, transformers\n",
    "* Asserts Ray version >= 2.47.1 for distributed features\n",
    "# MAGIC\n",
    "**Best Practices**: Version assertions prevent runtime errors in distributed setup\n",
    "# MAGIC\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "from packaging.version import Version\n",
    "\n",
    "import torch\n",
    "import flash_attn\n",
    "import vllm\n",
    "import ray\n",
    "import transformers\n",
    "\n",
    "print(torch.__version__, flash_attn.__version__, vllm.__version__, ray.__version__, transformers.__version__)\n",
    "assert Version(ray.__version__) >= Version(\"2.47.1\"), (\n",
    "    \"Ray version must be at least 2.47.1\"\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## Hugging Face Authentication\n",
    "**Display Name**: `===== LOGIN =====`\n",
    "# MAGIC\n",
    "**Purpose**: Authenticate with Hugging Face Hub for model access\n",
    "# MAGIC\n",
    "**Key Actions**: Uses `huggingface_hub.login()` for interactive authentication\n",
    "# MAGIC\n",
    "**Security**: Interactive login prevents hardcoding tokens (complies with FE workspace policy)\n",
    "# MAGIC\n",
    "---\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# DBTITLE 1,===== LOGIN =====\n",
    "# Login to hugging face\n",
    "from huggingface_hub import login\n",
    "\n",
    "login()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "## Cell 6: Ray Resource Reporting Function\n",
    "# MAGIC\n",
    "# MAGIC\n",
    "**Purpose**: Utility function to inspect Ray cluster resources and debug distributed setups\n",
    "# MAGIC\n",
    "**Key Functions**:\n",
    "* `print_ray_resources()`: Reports cluster resources and node details\n",
    "* Shows GPU allocation per node with specific GPU IDs\n",
    "* Graceful error handling for debugging\n",
    "# MAGIC\n",
    "**Usage**: Essential for verifying resource allocation matches expectations\n",
    "# MAGIC\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# DBTITLE 1,RAY REPORT\n",
    "import json\n",
    "\n",
    "def print_ray_resources():\n",
    "    try:\n",
    "        cluster_resources = ray.cluster_resources()\n",
    "        print(f\"Ray Cluster Resources: {json.dumps(cluster_resources, indent=2)}\")\n",
    "\n",
    "        nodes = ray.nodes()\n",
    "        print(f\"\\nDetected {len(nodes)} Ray nodes:\")\n",
    "        for node in nodes:\n",
    "            node_id = node.get(\"NodeID\", \"N/A\")\n",
    "            ip_address = node.get(\"NodeManagerAddress\", \"N/A\")\n",
    "            resources = node.get(\"Resources\", {})\n",
    "            num_gpus_ray = resources.get(\"GPU\", 0.0) # GPU resource is typically a float\n",
    "\n",
    "            print(f\"  Node ID: {node_id}, IP: {ip_address}\")\n",
    "            print(f\"    Ray-reported GPUs: {int(num_gpus_ray)}\") # Convert float to int for display\n",
    "            if \"GPU\" in resources and num_gpus_ray > 0:\n",
    "                # If specific GPU IDs are reported, show them\n",
    "                gpu_ids_on_node = [k for k, v in resources.items() if k.startswith(\"GPU_ID_\")]\n",
    "                if gpu_ids_on_node:\n",
    "                    print(f\"    Specific GPU IDs detected by Ray: {', '.join(gpu_ids_on_node)}\")\n",
    "            else:\n",
    "                print(f\"    No GPUs reported by Ray for this node.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while querying Ray cluster resources: {e}\")\n",
    "\n",
    "print_ray_resources()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# DBTITLE 1,===== MULI-NODE - CHAT =====\n",
    "\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "from packaging.version import Version\n",
    "from ray.util.scheduling_strategies import PlacementGroupSchedulingStrategy\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# def to_schema(item):\n",
    "#     messages = [\n",
    "#             {\"role\": \"system\", \"content\": \"You are a bot that responds with haikus.\"},\n",
    "#             {\"role\": \"user\", \"content\": item[\"item\"]},\n",
    "#         ]\n",
    "#     return {'item': messages}\n",
    "\n",
    "\n",
    "def scheduling_strategy_fn(tensor_parallel_size):\n",
    " \n",
    "    pg = ray.util.placement_group(\n",
    "        [{\n",
    "            \"GPU\": 1,\n",
    "            \"CPU\": 1\n",
    "        }] * tensor_parallel_size,\n",
    "        strategy=\"STRICT_PACK\",\n",
    "    )\n",
    "    return dict(scheduling_strategy=PlacementGroupSchedulingStrategy(\n",
    "        pg, placement_group_capture_child_tasks=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "from serverless_gpu.ray import ray_launch \n",
    "import ray\n",
    "import os\n",
    "from packaging.version import Version\n",
    "\n",
    "assert Version(ray.__version__) >= Version(\"2.22.0\"), \"Ray version must be at least 2.22.0\"\n",
    "\n",
    "class TaskRunner:\n",
    "    def run():\n",
    "        from typing import Any, Dict, List\n",
    "        import numpy as np\n",
    "        import ray\n",
    "        from vllm import LLM, SamplingParams\n",
    "\n",
    "        prompts = [\n",
    "            \"Hello, my name is\",\n",
    "            \"The president of the United States is\",\n",
    "            \"The future of AI is\",\n",
    "        ]\n",
    "        ds = ray.data.from_items(1000*prompts) #this was 100 but if you make it higher than each node is assured to get prompts\n",
    "\n",
    "        sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=100)\n",
    "\n",
    "        tensor_parallel_size = 1\n",
    "        num_instances = 5 \n",
    "\n",
    "        class LLMPredictor:\n",
    "            def __init__(self):\n",
    "                # \u2705 CHANGED: Switched to Non-FP8 model for stability on A10\n",
    "                self.llm = LLM(\n",
    "                    model=\"Qwen/Qwen3-4B-Instruct-2507\", # Removed -FP8\n",
    "                    tensor_parallel_size=1, \n",
    "                    dtype=\"bfloat16\",             # Native for A10\n",
    "                    trust_remote_code=True,       \n",
    "                    gpu_memory_utilization=0.90,  \n",
    "                    max_model_len=8192,           \n",
    "                    enable_prefix_caching=True,   \n",
    "                    enable_chunked_prefill=True, \n",
    "                    max_num_batched_tokens=8192,\n",
    "                )\n",
    "\n",
    "            def __call__(self, batch: Dict[str, np.ndarray]) -> Dict[str, list]:\n",
    "                outputs = self.llm.generate(batch[\"item\"], sampling_params)\n",
    "                prompt: List[str] = []\n",
    "                generated_text: List[str] = []\n",
    "                for output in outputs:\n",
    "                    prompt.append(output.prompt)\n",
    "                    generated_text.append(' '.join([o.text for o in output.outputs]))\n",
    "                return {\n",
    "                    \"prompt\": prompt,\n",
    "                    \"generated_text\": generated_text,\n",
    "                }\n",
    "\n",
    "        ds = ds.map_batches(\n",
    "            LLMPredictor,\n",
    "            concurrency=num_instances, \n",
    "            batch_size=32,\n",
    "            num_gpus=1,\n",
    "            num_cpus=12\n",
    "        )\n",
    "\n",
    "        outputs = ds.take(limit=10)\n",
    "        for output in outputs:\n",
    "            prompt = output[\"prompt\"]\n",
    "            generated_text = output[\"generated_text\"]\n",
    "            print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "\n",
    "@ray_launch(gpus=5, gpu_type='a10', remote=True)\n",
    "def run() -> None:\n",
    "    # os.environ['HF_TOKEN'] = 'hf_...' # Replace with your Hugging Face Token or use Databricks Secrets\n",
    "    # Example: os.environ['HF_TOKEN'] = dbutils.secrets.get(scope=\"my-scope\", key=\"hf-token\")\n",
    "\n",
    "    runner = TaskRunner.run()\n",
    "\n",
    "import os \n",
    "os.environ['RAY_TEMP_DIR'] = '/tmp/ray'\n",
    "run.distributed()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}