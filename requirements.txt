https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl
databricks-connect<16
transformers<4.54.0
vllm==0.8.5.post1
opentelemetry.exporter.prometheus
optree>=0.13.0
hf_transfer
numpy==1.26.4
serverless-gpu
packaging
ray[data]>=2.22.0